{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ba093f",
   "metadata": {},
   "source": [
    "# 0. Setting Up The Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44eb9d",
   "metadata": {},
   "source": [
    "## 0.1 Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f822ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.3 environment at: D:\\01 Work\\10-New-Learnings\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!uv pip install bs4 langchainhub langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f037c9",
   "metadata": {},
   "source": [
    "## 0.2 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62446f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788cd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0318ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05cb7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old Codes & Package\n",
    "\n",
    "# import bs4\n",
    "# from langchain import hub\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "# LangChain core\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Hub\n",
    "# from langchain import hub\n",
    "from langsmith import Client\n",
    "# Loaders & Vector DBs \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# OpenAI \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acb49ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617183d",
   "metadata": {},
   "source": [
    "## 0.3 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff92de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00d0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813f1b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c1180db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins lsv2_pt_\n"
     ]
    }
   ],
   "source": [
    "if langchain_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {langchain_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f004a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dca006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad6022",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97368655",
   "metadata": {},
   "source": [
    "## 1.1 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a3b94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9d22064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\01 Work\\10-New-Learnings\\.venv\\Lib\\site-packages\\google\\protobuf\\internal\\well_known_types.py:91: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0)\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1510cc",
   "metadata": {},
   "source": [
    "## 1.2 Retreival & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae3b2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a0d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50079065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c63002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps, allowing for easier execution and understanding. This approach can be implemented through various methods such as simple prompting, task-specific instructions, or relying on external classical planners. By decomposing tasks, agents can effectively plan ahead and navigate through the different stages of a task more efficiently.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e68a40",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500de92c",
   "metadata": {},
   "source": [
    "**Refere Doc**\n",
    "* [Count Tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
    "* [Token ~ 4 Chars](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
    "* [OpenAI Embeddings](https://docs.langchain.com/oss/python/integrations/text_embedding/openai)\n",
    "* [cl100k_base & cosine similarity](https://platform.openai.com/docs/guides/embeddings#faq)\n",
    "* ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "843b0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9c75d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c43e5393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cf32fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8806915835035412\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f067b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "524045d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11dfd635",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210744b",
   "metadata": {},
   "source": [
    "# 3. Retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2c48770",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "274fff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef2821",
   "metadata": {},
   "source": [
    "# 4. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e6d1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3a4390f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "158cf7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task Decomposition is a technique used by agents to break down complex tasks into smaller and simpler steps in order to plan ahead and enhance model performance. It involves transforming big tasks into multiple manageable tasks and exploring multiple reasoning possibilities at each step. This can be done through prompting techniques like Chain of Thought or Tree of Thoughts, as well as using task-specific instructions or human inputs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 315, 'total_tokens': 389, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CsuNfm76ATCLivJZCX9HXUAgqxHzW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b7579-d2ba-7a81-adb6-0c34b5fca07c-0', usage_metadata={'input_tokens': 315, 'output_tokens': 74, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Chain =  Prompt + LLM\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53014c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used by agents to break down complex tasks into smaller and simpler steps in order to plan ahead and enhance model performance. It involves transforming big tasks into multiple manageable tasks and exploring multiple reasoning possibilities at each step. This can be done through prompting techniques like Chain of Thought or Tree of Thoughts, as well as using task-specific instructions or human inputs.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f762a",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda653cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-learnings (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
