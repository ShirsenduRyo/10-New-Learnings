{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118c17ac",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"text-align: center;\">Query Transformation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d95aa",
   "metadata": {},
   "source": [
    "# 0. Setting Up The Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc8102",
   "metadata": {},
   "source": [
    "## 0.1 Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e6bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.3 environment at: D:\\01 Work\\10-New-Learnings\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 77ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!uv pip install bs4 langchainhub langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bc8dd",
   "metadata": {},
   "source": [
    "## 0.2 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe2a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36b45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e1b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "# LangChain core\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Hub\n",
    "from langsmith import Client\n",
    "# Loaders & Vector DBs \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# OpenAI \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85ed3b",
   "metadata": {},
   "source": [
    "## 0.3 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0fba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe1b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e227930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Langchain API Key exists and begins lsv2_pt_\n"
     ]
    }
   ],
   "source": [
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if langchain_api_key:\n",
    "    print(f\"Langchain API Key exists and begins {langchain_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Langchain API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2e7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5607b",
   "metadata": {},
   "source": [
    "# 1. Multi Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd688b",
   "metadata": {},
   "source": [
    "## 1.1 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e927d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c56eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8e50e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2317b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be216741",
   "metadata": {},
   "source": [
    "## 1.2 Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37db3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3d6e0",
   "metadata": {},
   "source": [
    "## 1.3 Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65fe0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c68c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6d07c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sport\\AppData\\Local\\Temp\\ipykernel_20168\\3271448290.py:8: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77e88f",
   "metadata": {},
   "source": [
    "## 1.4 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f263b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c37fa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to plan ahead and think step by step, transforming big tasks into multiple manageable tasks. Task decomposition can be done through various techniques such as Chain of Thought (CoT) and Tree of Thoughts, as well as using task-specific instructions or human inputs.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7b398",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dc54487",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d9963",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-learnings (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
